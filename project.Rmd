---
title: "Final Project"
author: "Alex Gonzalez, Hansen Zhang, Elvin Liu, Minh Tran"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r include = FALSE}
library(MASS)
library(ISLR2)
library(olsrr)
library(glmnet)
library(GGally)
library(corrplot)
library(tidyverse)
```

```{r}
df <- College
df_numeric <- df[, sapply(df, is.numeric)]

df_priv <- filter(df, Private == "Yes") # split data into private / public
df_pub <- filter(df, Private == "No")
```

```{r}
x <- model.matrix(F.Undergrad ~ ., df)[, -1]
y <- log(df$F.Undergrad)

# Check normality assumption of our outcome variable
hist(y)

n <- nrow(df)
eps <- 0.5
var <- mean(lm(y ~ x)$residuals^2)

# Simulate Y_tr conditional on the observed y
y_train <- rnorm(n = n, mean = eps * y, sd = sqrt(eps * (1-eps) * var))
y_test <- y - y_train
```

```{r}
# Lasso Regression I think?
grid <- 10^seq(10, -2, length = 100)

lasso.mod <- glmnet(x, y_train, alpha = 1, lambda = grid)

cv.out <- cv.glmnet(x, y_train, alpha = 1, lambda = grid)

bestlam <- cv.out$lambda.min

out <- glmnet(x, y, alpha = 1)
lasso.coef <- predict(out, type = "coefficients",
                      s = bestlam)[1:18, ]
lasso.coef

selected_indices <- (lasso.coef != 0)[2:length(lasso.coef)]
x_selected <- x[, selected_indices]
colnames(x_selected)

x_selected <- x_selected[, -c(2,7,8,9)]
fit_lm <- lm(y ~ x_selected)
summary(fit_lm)

x_selected <- x_selected[, -c(5,6,7)]

fit_lm <- lm(y ~ x_selected)
summary(fit_lm)
```

```{r}
boxplot(df$F.Undergrad)
boxplot(df$F.Undergrad ~ df$Private)

hist(df$F.Undergrad)
hist(df_priv$F.Undergrad)
hist(df_pub$F.Undergrad)

boxplot(log(df$F.Undergrad))
boxplot(log(df$F.Undergrad) ~ df$Private)

hist(log(df$F.Undergrad))
hist(log(df_priv$F.Undergrad))
hist(log(df_pub$F.Undergrad))
```

```{r}
response <- "F.Undergrad"
cor_mat <- cor(df_numeric)
corrplot(cor_mat, method = "color")
```

```{r}
# TRANSFORMATIONS AND BOXCOX
num_vars <- ncol(df_numeric)

ncols <- 3
nrows <- ceiling(num_vars / ncols)

for(col in names(df_numeric)) {
  x <- df_numeric[[col]]
  
  if (min(x, na.rm = TRUE) <= 0) {
    constant <- abs(min(x, na.rm = TRUE)) + 1
    x <- x + constant
  }
  
  boxcox(x ~ 1, lambda = seq(-2, 2, by = 0.1), plotit = TRUE)
  title(main = col)
}
```

```{r}
# Summary from Box-Cox

# Apps = 0 : [log]

# Accept = 0 : [log]

# Enroll = 0 : [log]

# Top10perc = 0 : [log] OR 0.5 : [sqrt] -- (NO?) DIFFERENCE

# Top25perc = 0.5 : [sqrt]

# F.Undergrad = -0.5 : [inverse] OR 0 : [log] -- kept [log] for simplicity

# P.Undergrad = 0 : [log]

# Outstate = 0.5 : [sqrt]

# Room.Board = 0 : [log] OR 0.5 : [sqrt] -- ? DIFFERENCE

# Books = 0 : [log]

# Personal = 0 : [log]

# PhD = 2 : [sq]

# Terminal = 2 : [sq]

# S.F.Ratio = 0.5 : [sqrt]

# perc.alumni = 0.5 : [sqrt]

# Expend = -0.5 : [inverse]

# Grad.Rate = 1 : [none]

names(College)
```

```{r}
plot(log(df$Top10perc), log(df$P.Undergrad)) # log Top10perc
plot(sqrt(df$Top10perc), log(df$P.Undergrad)) # sqrt Top10perc

plot(log(df$Room.Board), log(df$P.Undergrad)) # log Room.Board
plot(sqrt(df$Room.Board), log(df$P.Undergrad)) # sqrt Room.Board
```

```{r}
# Top10perc is log or sqrt
# Room.Board is log or sqrt
# Different combinations of transformations for Top10perc and Room.Board
transform_combinations <- list(
  list(Top10perc = log,  Room.Board = log),
  list(Top10perc = log,  Room.Board = sqrt),
  list(Top10perc = sqrt, Room.Board = log),
  list(Top10perc = sqrt, Room.Board = sqrt)
)

apply_transformation <- function(transform) {
  df %>%
    transmute(
      across(c(Apps, Accept, Enroll, F.Undergrad, P.Undergrad, Books, Personal), log, .names = "{.col}_log"),
      across(c(Top25perc, Outstate, S.F.Ratio, perc.alumni), sqrt, .names = "{.col}_sqrt"),
      across(c(PhD, Terminal), ~ .x^2, .names = "{.col}_sq"),
      Private = Private,
      Grad.Rate = Grad.Rate,
      Top10perc_new = transform$Top10perc(Top10perc),
      Room.Board_new = transform$Room.Board(Room.Board),
      Expend_inv = (Expend^(-0.5) - 1) / (-0.5) # lambda = -0.5
    )
}

# list of df's with transformed variables
df_list <- map(transform_combinations, apply_transformation)

df_transform_list <- lapply(df_list, identity)
names(df_transform_list) <- paste0("df_transform_", seq_along(df_list))

for (i in seq_along(df_list)) {
  assign(paste0("df_transform_", i), df_list[[i]])
}

# Note: Top10perc_new and Room.Board_new in df_transform_i are both transformed
# variables, I just couldn't figure out how to change the "new" in their names
# to their respective transformations like "log" or "sqrt"
```

```{r}
main_effects_models <- lapply(df_list, function(df) {
  lm(F.Undergrad_log ~ Private + Apps_log + Accept_log + Enroll_log + 
       Top10perc_new + Top25perc_sqrt + P.Undergrad_log + Outstate_sqrt + 
       Room.Board_new + Books_log + Personal_log + PhD_sq + Terminal_sq + 
       S.F.Ratio_sqrt + perc.alumni_sqrt + Expend_inv + Grad.Rate, 
     data = df)
})

# main_effects_log_log is main effects but log(Top10perc) and log(Room.Board)
# main_effects_log_sqrt is main effects but log(Top10perc) and sqrt(Room.Board)
# main_effects_sqrt_log is main effects but sqrt(Top10perc) and log(Room.Board)
# main_effects_sqrt_sqrt is main effects but sqrt(Top10perc) and sqrt(Room.Board)
main_effects_log_log <- main_effects_models[[1]]
main_effects_log_sqrt <- main_effects_models[[2]]
main_effects_sqrt_log <- main_effects_models[[3]]
main_effects_sqrt_sqrt <- main_effects_models[[4]]

(main_sum_log_log <- summary(main_effects_log_log)) #     0.9488365
(main_sum_log_sqrt <- summary(main_effects_log_sqrt)) #   0.9488282
(main_sum_sqrt_log <- summary(main_effects_sqrt_log)) #   0.9488441
(main_sum_sqrt_sqrt <- summary(main_effects_sqrt_sqrt)) # 0.9488363

# Extremely marginal differences
```

```{r}
AIC(main_effects_log_log, main_effects_log_sqrt, main_effects_sqrt_log, main_effects_sqrt_sqrt)

# main_effects_sqrt_log has the smallest AIC, so we choose this as our main effects model

fit_main <- main_effects_sqrt_log
```

```{r}
# EVALUATING MODEL WITH P VALUE CORRECTIONS
# alpha = 0.05

pvals <- sort(main_sum_sqrt_log$coefficients[2:18, 4])

# Bonferroni
# Holm
# Hockberg
# Franklin Delano Roosevelt
# Benjamin
which(p.adjust(pvals, method = "bonferroni") < 0.05)
which(p.adjust(pvals, method = "holm") < 0.05)
which(p.adjust(pvals, method = "hochberg") < 0.05)
which(p.adjust(pvals, method = "fdr") < 0.05)
which(p.adjust(pvals, method = "BH") < 0.05)

# bonferroni, holm, and hochberg give PrivateYes, Enroll_log, and P.Undergrad_log
# fdr and BH give those and also Top25perc_sqrt, S.F.Ratio_sqrt, and perc.alumni_sqrt

thin.model <- lm(F.Undergrad_log ~ Private + Enroll_log + P.Undergrad_log, 
                 data = df_transform_3)

fat.model <- lm(F.Undergrad_log ~ Private + Enroll_log + Top25perc_sqrt + 
                  P.Undergrad_log + S.F.Ratio_sqrt + perc.alumni_sqrt, 
                data = df_transform_3)

anova(thin.model, fat.model)

# forward selection starting from thin.model and ending at fat.model
# iteratively add each variable to identify which of the 3 variables contribute
step(thin.model,
     scope = list(lower = thin.model, upper = fat.model), 
     direction = "forward")

# AIC = -2198.26
# AIC = -2223.1 add Top25perc_sqrt
# AIC = -2229.73 add perc.alumni_sqrt
# AIC = -2231.76 add S.F.Ratio_sqrt
# S.F.Ratio_sqrt barely contributes

# best main effects are Private, Enroll_log, P.Undergrad_log, Top25perc_sqrt,
# and perc.alumni_sqrt
parsimony <- lm(F.Undergrad_log ~ Private + Enroll_log + P.Undergrad_log + 
                  Top25perc_sqrt + perc.alumni_sqrt, 
                data = df_transform_3)
summary(parsimony)

pval_ordered <- sort(pvals)
rank <- 1:length(pval_ordered)
alpha <- 0.05
m <- length(pval_ordered)

bonferroni <- alpha / m
holm <- alpha / (m - rank + 1)
hochberg <- (rank / m) * alpha

plot <- data.frame(
  Rank = rank, 
  PValue = pval_ordered, 
  Bonferroni = bonferroni, 
  Holm = holm, 
  Benjamini_Hochberg = hochberg
)

p <- ggplot(head(plot, 8), aes(x = Rank, y = PValue)) + 
  geom_point() + 
  geom_line(aes(y = Bonferroni), color = "red", linetype = "dashed") + 
  geom_line(aes(y = Holm), color = "blue", linetype = "dotted") + 
  geom_line(aes(y = Benjamini_Hochberg), color = "green", linetype = "dotdash") + 
  labs(x = "Rank", y = "P-value") + 
  theme_minimal()

p

# the code below just does the forward stepwise selection but manually
# 
# # statistically significant!
# # do it again!
# 
# p.vals_2 <- summary(fat_model_1)$coefficients[2:7, 4]
# 
# which(p.adjust(p.vals_2, method = "bonferroni") < 0.05)
# which(p.adjust(p.vals_2, method = "holm") < 0.05)
# which(p.adjust(p.vals_2, method = "hochberg") < 0.05)
# which(p.adjust(p.vals_2, method = "fdr") < 0.05)
#
# bonferroni gives PrivateYes, Enroll_log, Top25perc_sqrt, and P.Undergrad_log
# holm, hochberg, and fdr give those and also S.F.Ratio_sqrt and perc.alumni_sqrt

# thin_model_2 <- lm(F.Undergrad_log ~ Private + Enroll_log + Top25perc_sqrt + 
#                      P.Undergrad_log,
#                    data = df_transform_3)
# fat_model_2 <- lm(F.Undergrad_log ~ Private + Enroll_log + Top25perc_sqrt + 
#                     P.Undergrad_log + S.F.Ratio_sqrt + perc.alumni_sqrt,
#                   data = df_transform_3)
# anova(thin_model_2, fat_model_2)
# 
# # statistically significant!
# # in addition to Private, Enroll_log, Top25perc_sqrt, and P-undergrad_log
# # either S.F.Ratio_sqrt is important, perc.alumni_sqrt is important, or both
# 
# x <- lm(F.Undergrad_log ~ Private + Enroll_log + Top25perc_sqrt + 
#                     P.Undergrad_log + perc.alumni_sqrt,
#                   data = df_transform_3)
# 
# anova(x, fat_model_2) # S.F.Ratio_sqrt is not sig
# 
# y <- lm(F.Undergrad_log ~ Private + Enroll_log + Top25perc_sqrt + 
#                     P.Undergrad_log + S.F.Ratio_sqrt,
#                   data = df_transform_3)
# 
# anova(y, fat_model_2) # perc is sig
```

```{r}
# main effects + interactions of our newly trimmed model with best main effects
trimmed_full_model <- lm(F.Undergrad_log ~ (Private + Enroll_log + 
                                           P.Undergrad_log + Top25perc_sqrt + 
                                           perc.alumni_sqrt)^2, 
                         data = df_transform_3)
summary(trimmed_full_model)

# Correcting the p-values that we get from the summary
p.vals_trim <- sort(summary(trimmed_full_model)$coefficients[2:11 ,4])

which(p.adjust(p.vals_trim, method = "bonferroni") < 0.05)
which(p.adjust(p.vals_trim, method = "holm") < 0.05)
which(p.adjust(p.vals_trim, method = "hochberg") < 0.05)
which(p.adjust(p.vals_trim, method = "fdr") < 0.05)

# bonf, holm, hoch find Enroll_log, P.Undergrad_log, Private:P.Undergrad_log
# by extension finds Private significant
# fdr finds the addition of PrivateYes:Top25perc_sqrt and by extension 
# Top25perc_sqrt
```

```{r}
# Interaction model
thin_inter_model <- lm(F.Undergrad_log ~ Private + Enroll_log + 
                         P.Undergrad_log + Private:P.Undergrad_log, 
                       data = df_transform_3)

# add Top25perc_sqrt
mid_inter_model <- lm(F.Undergrad_log ~ Private + Enroll_log + 
                           P.Undergrad_log + Top25perc_sqrt + 
                           Private:P.Undergrad_log, 
                         data = df_transform_3)

# add Top25perc_sqrt
fat_inter_model <- lm(F.Undergrad_log ~ Private + Enroll_log + 
                        P.Undergrad_log + Top25perc_sqrt + 
                        Private:P.Undergrad_log + Private:Top25perc_sqrt, 
                       data = df_transform_3)

AIC(thin_inter_model, mid_inter_model, fat_inter_model)
# intermediate model is the best

final_model <- mid_inter_model
```

```{r}
# The full model with interactions between every single parameter has Adjusted
# R-squared of 0.9628 and R-squared of 0.9701

# The sparse model on the other hand has... Adjusted R-squared (0.9466)
full_model <- lm(F.Undergrad_log ~ (Private + Apps_log + Accept_log + 
                                    Enroll_log + Top10perc_new + 
                                    Top25perc_sqrt + P.Undergrad_log + 
                                    Outstate_sqrt + Room.Board_new + Books_log + 
                                    Personal_log + PhD_sq + Terminal_sq + 
                                    S.F.Ratio_sqrt + perc.alumni_sqrt + 
                                    Expend_inv + Grad.Rate)^2,
                 data = df_transform_3)

summary(full_model)
summary(final_model)
```
